slurmstepd: info: Setting TMPDIR to /scratch/13087188. Previous errors about TMPDIR can be discarded
dwi_sin
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(10,)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.48382686 -0.4901889  -0.48976722 -0.49093971 -0.47782143 -0.4856415
 -0.47456548 -0.47882567 -0.48365626 -0.4975566  -0.48223779 -0.50135112
 -0.48045748 -0.48606925 -0.47773888 -0.48046602 -0.48128146 -0.49320964
 -0.48476348 -0.49664197 -0.48606856 -0.49472168 -0.48427686 -0.49048415
 -0.48473285 -0.49797304 -0.48212551 -0.50217224 -0.48810568 -0.48931777
 -0.4842737  -0.48624924 -0.46888611 -0.47597606 -0.47413666 -0.47521008
 -0.4702531  -0.47111564 -0.47916378 -0.47606551 -0.47262483 -0.47150192
 -0.47233345 -0.48030649 -0.47323685 -0.47328006 -0.47464607 -0.47981704]
[0.05592532 0.06011481 0.05587643 0.05697301 0.05584149 0.06544969
 0.06108637 0.05225516 0.05221457 0.06547985 0.06315531 0.05748929
 0.0540535  0.06887143 0.06394577 0.05709712 0.05979936 0.05473419
 0.05407668 0.06141923 0.0581262  0.06849208 0.05360624 0.06889059
 0.05983663 0.05290725 0.05215185 0.06751296 0.05616161 0.05983722
 0.05947273 0.06259223 0.05631355 0.05814578 0.05750097 0.05521058
 0.05887611 0.05943767 0.05599575 0.05714179 0.05638008 0.05844108
 0.05728308 0.06413292 0.05297914 0.06122148 0.05296671 0.0530634 ]
[27 39 38 41 16 32 10 17 26 45 25 47 21 34 15 22 23 42 31 44 33 43 29 40
 30 46 24 48 36 37 28 35  1 13  9 12  2  3 18 14  6  4  5 20  7  8 11 19]
dwi_cos
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(10,)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.4299354  -0.44582157 -0.43079475 -0.43499556 -0.42534023 -0.43187148
 -0.41966504 -0.43805746 -0.42563623 -0.44686846 -0.43384964 -0.43753956
 -0.42983069 -0.44058659 -0.42869485 -0.42591555 -0.43634929 -0.43689586
 -0.43598863 -0.43753157 -0.43529531 -0.43508184 -0.43087573 -0.4313864
 -0.4334653  -0.44545901 -0.43714661 -0.43404069 -0.43215659 -0.45013439
 -0.43327248 -0.44210138 -0.41987959 -0.41807896 -0.42509047 -0.42443865
 -0.41886689 -0.41630933 -0.42716246 -0.42705948 -0.41712731 -0.42039078
 -0.42145334 -0.42310725 -0.419276   -0.42055931 -0.42274934 -0.42959239]
[0.04142444 0.04393249 0.0458461  0.04543813 0.04491883 0.04077277
 0.0278073  0.04490341 0.04075082 0.0435486  0.04143274 0.04582912
 0.0337889  0.03683317 0.04375491 0.04623294 0.04427134 0.03975532
 0.03784931 0.0434332  0.04067983 0.0296886  0.03393473 0.03874331
 0.03881165 0.03110778 0.03882404 0.03797176 0.03500348 0.03264107
 0.03932071 0.03575273 0.03444888 0.03850549 0.04064332 0.04355239
 0.03278929 0.03665159 0.03678055 0.03321332 0.04101495 0.03989017
 0.0379903  0.03213836 0.0411508  0.03998272 0.04059098 0.04201047]
[23 46 24 33 15 27  6 42 16 47 31 41 22 43 20 17 37 38 36 40 35 34 25 26
 30 45 39 32 28 48 29 44  7  3 14 13  4  1 19 18  2  8 10 12  5  9 11 21]
wind_max
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(10,)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-2.47550439 -2.50400934 -2.45273466 -2.54060227 -2.45580795 -2.52847616
 -2.40858624 -2.52301715 -2.49085641 -2.54162582 -2.45549283 -2.60449955
 -2.44285527 -2.55584742 -2.45606921 -2.53778006 -2.44211183 -2.58977829
 -2.48671492 -2.70553808 -2.38579215 -2.59399632 -2.54595041 -2.73280482
 -2.40930519 -2.55651566 -2.46575556 -2.66159912 -2.40884056 -2.58327247
 -2.48020783 -2.69909424 -2.40256674 -2.46257514 -2.45558995 -2.52856065
 -2.38822786 -2.44947818 -2.45614433 -2.52638076 -2.38145421 -2.45528654
 -2.4493296  -2.51832173 -2.36518034 -2.42372367 -2.41683084 -2.50846303]
[0.43357736 0.36623876 0.42178614 0.4018352  0.39336572 0.42113335
 0.42076849 0.38711399 0.41500246 0.44114921 0.44461079 0.37085171
 0.4081058  0.41303979 0.4131852  0.38938699 0.47559713 0.36578515
 0.44853519 0.38175363 0.43125915 0.34931761 0.40552552 0.3681417
 0.46739441 0.40021898 0.45416735 0.38062498 0.39923833 0.41371955
 0.43993771 0.41138158 0.43740842 0.37571383 0.45095    0.39189367
 0.45702898 0.37873997 0.42406087 0.39835917 0.42961585 0.38483385
 0.48137759 0.38681224 0.42118151 0.3768156  0.42447608 0.35752902]
[24 28 15 36 19 33  6 31 27 37 17 44 12 39 20 35 11 42 26 47  3 43 38 48
  8 40 23 45  7 41 25 46  5 22 18 34  4 14 21 32  2 16 13 30  1 10  9 29]
wind_med
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(10,)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-2.13403262 -2.15145759 -2.09663719 -2.15702573 -2.13408921 -2.15043466
 -2.09936319 -2.11572358 -2.14401099 -2.11352082 -2.1592664  -2.17755111
 -2.09677914 -2.17093999 -2.12167869 -2.13985024 -2.13392485 -2.22251603
 -2.18782962 -2.33562699 -2.15473849 -2.22739659 -2.18680731 -2.35671282
 -2.1285351  -2.21638078 -2.17995036 -2.3396833  -2.12838549 -2.2249771
 -2.22355225 -2.39282855 -2.0874948  -2.11283211 -2.10486496 -2.15511971
 -2.07212695 -2.09583243 -2.1169371  -2.14164817 -2.07694827 -2.11881434
 -2.12476611 -2.14668932 -2.06319242 -2.10951576 -2.15471422 -2.15544835]
[0.40371841 0.44025977 0.40524136 0.4080252  0.41214996 0.40725308
 0.4064339  0.37921172 0.40872434 0.38203773 0.42447692 0.37588114
 0.42135962 0.40567216 0.44583595 0.39642391 0.46481542 0.37258183
 0.46329204 0.41739204 0.42363686 0.36946764 0.41694451 0.36783384
 0.46466872 0.41749554 0.45062875 0.40865158 0.44531484 0.43152845
 0.46227286 0.40503084 0.45334209 0.40077365 0.45285893 0.41776881
 0.44993689 0.39782996 0.4256151  0.39407101 0.45609548 0.38202139
 0.44925056 0.40427354 0.44715976 0.40295716 0.47665016 0.39660773]
[21 28  6 33 22 27  8 13 25 12 34 36  7 35 16 23 20 41 39 45 30 44 38 47
 19 40 37 46 18 43 42 48  4 11  9 31  2  5 14 24  3 15 17 26  1 10 29 32]
shww_max
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(10,)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.92991442 -0.95748314 -0.94016222 -0.97071096 -0.9175756  -0.9572039
 -0.91834558 -0.96680523 -0.95168982 -0.96349318 -0.94167082 -0.99376178
 -0.92164409 -0.95880333 -0.90412131 -0.97083358 -0.99945361 -1.02649272
 -0.99822844 -1.1066599  -0.97795521 -1.02420316 -1.02770137 -1.06991446
 -0.96765003 -1.03934178 -1.03530886 -1.09531277 -0.96375958 -1.00814455
 -1.0118873  -1.09414724 -0.93915488 -0.96728712 -0.95540406 -1.01402069
 -0.91451193 -0.96523126 -0.96716097 -1.00589334 -0.9306059  -0.95680703
 -0.94900636 -1.00411908 -0.92714381 -0.95764427 -0.95598247 -1.03594303]
[0.37928839 0.37373465 0.38422937 0.37526656 0.36743017 0.37417535
 0.38176887 0.39257369 0.38093609 0.34947874 0.36605502 0.3828425
 0.38523733 0.35344182 0.35766978 0.38674225 0.39630856 0.37763141
 0.39369115 0.38188527 0.39448019 0.39041547 0.38450239 0.33168289
 0.38442449 0.37333737 0.38987017 0.38918786 0.37406402 0.36317311
 0.40467166 0.35688655 0.37662174 0.36764195 0.39356426 0.36120308
 0.38292528 0.38379988 0.38249795 0.37379399 0.38017082 0.35807295
 0.39763344 0.363366   0.39023824 0.3586235  0.40822595 0.37550471]
[ 7 18 10 28  3 17  4 24 13 21 11 31  5 20  1 29 33 40 32 48 30 39 41 45
 27 44 42 47 22 36 37 46  9 26 14 38  2 23 25 35  8 16 12 34  6 19 15 43]
shww_med
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(10,)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.71219116 -0.74542142 -0.71307285 -0.73795688 -0.7149996  -0.75223328
 -0.70565067 -0.72661655 -0.71727241 -0.74278657 -0.69629108 -0.76189463
 -0.71950327 -0.7375435  -0.69775378 -0.73429052 -0.73656342 -0.80545358
 -0.76065875 -0.82402467 -0.74026994 -0.78130836 -0.75932477 -0.83472826
 -0.73511911 -0.77022404 -0.76149694 -0.80911355 -0.75363369 -0.80664627
 -0.76903859 -0.83478936 -0.71029943 -0.7350408  -0.72559204 -0.7702503
 -0.70845767 -0.73361087 -0.74833075 -0.77978491 -0.69936379 -0.7284464
 -0.72852032 -0.76853479 -0.71213205 -0.74345687 -0.72628902 -0.78889437]
[0.31208137 0.30785328 0.3068047  0.32201877 0.31771343 0.30884909
 0.30371712 0.32048522 0.30039618 0.28882996 0.30662742 0.28822218
 0.30428801 0.29063218 0.32070419 0.30847823 0.33259161 0.30125259
 0.33306023 0.28580751 0.31498262 0.30626967 0.30810672 0.29531329
 0.33886619 0.29064524 0.32472942 0.28720584 0.33312233 0.31627368
 0.32028526 0.28559426 0.3156269  0.30742126 0.31999593 0.29725012
 0.30607883 0.29750475 0.31907501 0.3017337  0.30369601 0.30429479
 0.30245204 0.29839344 0.31749369 0.28598121 0.32751052 0.28335287]
[ 8 28  9 24 10 30  4 15 11 26  1 35 12 23  2 19 22 43 33 46 25 41 32 47
 21 38 34 45 31 44 37 48  6 20 13 39  5 18 29 40  3 16 17 36  7 27 14 42]
mdts_sin
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(10,)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.09963538 -0.1114898  -0.09979516 -0.11689535 -0.09637054 -0.11035405
 -0.0913172  -0.09293874 -0.09944136 -0.11490851 -0.10046513 -0.10669357
 -0.09496779 -0.10282689 -0.09233858 -0.09152144 -0.0955064  -0.10530562
 -0.09367742 -0.10350746 -0.09335274 -0.10105973 -0.0944197  -0.09668642
 -0.09309905 -0.11145878 -0.09535857 -0.10021946 -0.09262037 -0.09879268
 -0.09338526 -0.09425748 -0.08940199 -0.08900187 -0.09400346 -0.09601105
 -0.09378712 -0.09430138 -0.09492212 -0.09441323 -0.0903544  -0.09174091
 -0.09244442 -0.09240961 -0.09192055 -0.08962647 -0.09230309 -0.09278781]
[0.04105633 0.04235366 0.04480062 0.0456754  0.04698901 0.05275245
 0.04928733 0.04227941 0.04625611 0.04493255 0.04422076 0.04084205
 0.04402933 0.03847946 0.04663483 0.04589652 0.04682499 0.04172512
 0.04604386 0.04098052 0.04410743 0.04348505 0.04721844 0.04472946
 0.04566989 0.04542873 0.04505073 0.04109736 0.04564307 0.04521581
 0.04656968 0.04431929 0.04575948 0.04181377 0.04850359 0.04325617
 0.04769332 0.04678693 0.04580765 0.04264105 0.04487237 0.04517895
 0.04603337 0.04357708 0.044539   0.04551501 0.04612643 0.04362288]
[35 46 36 48 31 44  5 15 34 47 38 43 27 40 10  6 29 42 19 41 17 39 25 32
 16 45 28 37 13 33 18 22  2  1 21 30 20 23 26 24  4  7 12 11  8  3  9 14]
mdts_cos
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(10,)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.1053361  -0.11491646 -0.1051823  -0.12005913 -0.1041783  -0.11570194
 -0.10612654 -0.1048668  -0.10758598 -0.12210518 -0.10977438 -0.11575479
 -0.1038261  -0.1169477  -0.10070385 -0.10219929 -0.10037764 -0.1164522
 -0.10159746 -0.11002079 -0.10089927 -0.11035753 -0.0998687  -0.10820258
 -0.10340005 -0.11425531 -0.10049467 -0.11222522 -0.09998377 -0.11178423
 -0.10048607 -0.103135   -0.09975976 -0.10101146 -0.10117836 -0.10991882
 -0.09961387 -0.10251319 -0.10020409 -0.10799355 -0.09752341 -0.10357188
 -0.10284355 -0.10711923 -0.0984209  -0.1012806  -0.1016     -0.10031019]
[0.03751861 0.03999966 0.04232095 0.04372068 0.03905368 0.04288991
 0.03645909 0.0388651  0.03607356 0.04124657 0.0374257  0.04194305
 0.04060744 0.04488871 0.04062129 0.03905221 0.03944075 0.03825457
 0.03850527 0.04144772 0.03817809 0.0348468  0.04038661 0.03894876
 0.03886556 0.03257106 0.03926008 0.03770684 0.04029938 0.03843943
 0.03977053 0.03511221 0.03988765 0.03785599 0.04136956 0.03486308
 0.03881019 0.03845993 0.0394576  0.03642554 0.03892278 0.03786309
 0.04010809 0.03973066 0.03960083 0.03867026 0.03944948 0.03681476]
[29 42 28 47 26 43 30 27 32 48 35 44 25 46 12 19  9 45 17 37 13 38  5 34
 23 41 11 40  6 39 10 22  4 14 15 36  3 20  7 33  1 24 21 31  2 16 18  8]
shts_max
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(10,)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.58059198 -0.63406976 -0.586164   -0.60977672 -0.57313505 -0.6244996
 -0.56895423 -0.60731159 -0.58628059 -0.62689327 -0.57929466 -0.61501333
 -0.57642267 -0.59616894 -0.56433051 -0.59308069 -0.60325159 -0.64753384
 -0.64154894 -0.68881819 -0.62948334 -0.64090901 -0.63411374 -0.71117044
 -0.62030175 -0.65599819 -0.64401515 -0.68126233 -0.60336081 -0.64964336
 -0.64261819 -0.68894673 -0.58243107 -0.5959284  -0.58269138 -0.61045719
 -0.5784888  -0.58751722 -0.59181482 -0.62292716 -0.58188804 -0.58800955
 -0.5908752  -0.6121674  -0.58383996 -0.58607214 -0.60069298 -0.63225197]
[0.14337543 0.12686181 0.14821252 0.13132904 0.13949132 0.12628447
 0.15172779 0.13483274 0.14643724 0.12523183 0.1404796  0.12610674
 0.13745384 0.12987065 0.15049518 0.14525757 0.13313134 0.12714107
 0.149769   0.13274756 0.1541881  0.13086711 0.15429762 0.13765851
 0.16463242 0.14131521 0.14358157 0.1268816  0.14138523 0.12795762
 0.15946282 0.14208768 0.15452617 0.14540449 0.15309851 0.14696583
 0.15492349 0.14154545 0.15905622 0.14919695 0.1466654  0.14451259
 0.1443569  0.13454284 0.15181705 0.14332325 0.16511067 0.15531807]
[ 7 36 13 26  3 32  2 25 14 33  6 29  4 21  1 19 23 42 39 46 34 38 37 48
 30 44 41 45 24 43 40 47  9 20 10 27  5 15 18 31  8 16 17 28 11 12 22 35]
shts_med
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(10,)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.51067813 -0.55178147 -0.50237141 -0.55474762 -0.50439413 -0.53666916
 -0.50288354 -0.53093991 -0.51856758 -0.53994243 -0.51339877 -0.55582222
 -0.49794051 -0.54064335 -0.51550499 -0.52447705 -0.53813092 -0.56642893
 -0.56767578 -0.60223718 -0.53620882 -0.56925929 -0.57068319 -0.61335426
 -0.5389787  -0.56579997 -0.55490514 -0.59546859 -0.53003422 -0.57567025
 -0.55679659 -0.59779411 -0.51578796 -0.5249891  -0.51864323 -0.54156344
 -0.50597008 -0.52391939 -0.52769147 -0.54300692 -0.51434781 -0.52885431
 -0.52948805 -0.53204894 -0.50774596 -0.51911943 -0.53274255 -0.54039464]
[0.10795821 0.10574601 0.10839907 0.0919808  0.10973818 0.10379623
 0.11796089 0.10572995 0.1109898  0.09732766 0.11772982 0.10318295
 0.11658161 0.10458304 0.11088395 0.10190786 0.09760114 0.11168735
 0.11804919 0.11443027 0.12003057 0.09006995 0.13156695 0.09168127
 0.11624593 0.09249575 0.12091296 0.09440446 0.10468907 0.09067733
 0.11528455 0.10426863 0.11393246 0.10664541 0.11796103 0.10496677
 0.11730406 0.09150018 0.11421736 0.09864317 0.12162643 0.10521805
 0.126394   0.10805831 0.11228969 0.10722185 0.12920256 0.10829308]
[ 7 34  2 35  4 26  3 22 12 29  8 37  1 31 10 16 27 40 41 47 25 42 43 48
 28 39 36 45 21 44 38 46 11 17 13 32  5 15 18 33  9 19 20 23  6 14 24 30]

*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 13087188)                   *
*                                                                           *
*****************************************************************************

Job ID: 13087188
Cluster: finisterrae3
User/Group: curso342/ulc
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 32
CPU Utilized: 11:28:39
CPU Efficiency: 95.29% of 12:02:40 core-walltime
Job Wall-clock time: 00:22:35
Memory Utilized: 8.49 GB
Memory Efficiency: 53.08% of 16.00 GB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************

