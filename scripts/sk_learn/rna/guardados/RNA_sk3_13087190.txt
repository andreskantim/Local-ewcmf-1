slurmstepd: info: Setting TMPDIR to /scratch/13087190. Previous errors about TMPDIR can be discarded
dwi_sin
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(30, 10)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.50033362 -0.51068914 -0.49268324 -0.51721764 -0.49793658 -0.50814744
 -0.48393743 -0.48092974 -0.50053012 -0.51453048 -0.49570151 -0.50374614
 -0.49934365 -0.5075767  -0.47669805 -0.5031011  -0.5013219  -0.51593228
 -0.50649963 -0.50723152 -0.49315923 -0.504265   -0.49685296 -0.50328801
 -0.4978865  -0.5092197  -0.49968339 -0.49774553 -0.49430593 -0.52432589
 -0.49123082 -0.50158431 -0.46894303 -0.48080376 -0.47969313 -0.47647837
 -0.47009654 -0.47382682 -0.47802011 -0.48564929 -0.47034936 -0.47532741
 -0.47929056 -0.488527   -0.46940519 -0.47605047 -0.47691938 -0.49348393]
[0.0475328  0.06402315 0.05519092 0.06874379 0.06603028 0.06641063
 0.06166601 0.05318472 0.06401489 0.05873247 0.05025385 0.07527645
 0.0575927  0.06744623 0.06192228 0.06131252 0.04736021 0.05410462
 0.05784047 0.06780224 0.05406887 0.05123034 0.04972825 0.07105593
 0.05393216 0.06245531 0.06029618 0.06508303 0.05682621 0.07185161
 0.05623053 0.06101477 0.05660501 0.06649425 0.06646308 0.05914318
 0.05952784 0.06159387 0.05509636 0.05410394 0.06027965 0.05931978
 0.06119825 0.0651391  0.05933973 0.05900331 0.05672256 0.06123352]
[31 44 20 47 28 42 16 15 32 45 24 37 29 41  9 35 33 46 39 40 21 38 25 36
 27 43 30 26 23 48 19 34  1 14 13  8  3  5 11 17  4  6 12 18  2  7 10 22]
dwi_cos
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(30, 10)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.44392769 -0.46305061 -0.4406084  -0.45679788 -0.44983889 -0.45793052
 -0.43750293 -0.43885573 -0.45530479 -0.45685896 -0.43771387 -0.44088448
 -0.44570218 -0.44342109 -0.42922911 -0.43511807 -0.44710149 -0.45257274
 -0.44621869 -0.45838828 -0.45366814 -0.45786712 -0.44696584 -0.45147199
 -0.44317727 -0.46787932 -0.44227948 -0.45775936 -0.44660834 -0.44703688
 -0.45116497 -0.45269275 -0.41914988 -0.42829976 -0.4334018  -0.43785291
 -0.42315826 -0.42247174 -0.42984341 -0.43760533 -0.41787576 -0.42029949
 -0.42849075 -0.43978275 -0.41984187 -0.42669767 -0.4321142  -0.43629802]
[0.03456065 0.03457509 0.0460474  0.04764395 0.04776643 0.04118387
 0.03514403 0.0428575  0.06502847 0.04418795 0.04815998 0.04365107
 0.04907463 0.03325482 0.04094254 0.04243009 0.0447908  0.03567994
 0.03495964 0.04144084 0.03881003 0.03988103 0.037975   0.03681813
 0.04298738 0.03261752 0.04287564 0.0429282  0.03747062 0.03868384
 0.03390718 0.03811501 0.03720027 0.03657117 0.03912243 0.0372119
 0.04259638 0.03721159 0.04800808 0.03976516 0.03977486 0.03667728
 0.03860458 0.04265911 0.04305047 0.03998046 0.02736697 0.03151337]
[27 47 22 41 34 45 16 20 40 42 18 23 28 26 10 14 33 37 29 46 39 44 31 36
 25 48 24 43 30 32 35 38  2  8 13 19  6  5 11 17  1  4  9 21  3  7 12 15]
wind_max
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(30, 10)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-2.43077656 -2.49881812 -2.41005371 -2.46454295 -2.43544902 -2.51994459
 -2.37642358 -2.49599933 -2.46393553 -2.50070123 -2.43324541 -2.56004392
 -2.42361785 -2.47414237 -2.43387877 -2.50086121 -2.63080872 -2.9944763
 -2.69406633 -2.77380896 -2.56344408 -2.78752188 -2.57971313 -2.67628129
 -2.70592615 -2.87927959 -2.64154835 -2.89470674 -2.61456766 -2.78416767
 -2.51089999 -2.7466313  -2.43728299 -2.54400967 -2.45161203 -2.5134883
 -2.37160693 -2.48324637 -2.47581033 -2.52556051 -2.42730367 -2.52432945
 -2.48866594 -2.51359066 -2.41401152 -2.46055423 -2.41600626 -2.52547244]
[0.40717866 0.36728171 0.39582948 0.35628512 0.37657962 0.35815655
 0.40472132 0.35932076 0.37284017 0.39850699 0.40161277 0.37617326
 0.38609238 0.38381596 0.41270031 0.35165186 0.38065516 0.42276527
 0.46074427 0.44208089 0.41542651 0.34067    0.423571   0.38262179
 0.42336655 0.43967807 0.41436276 0.44440434 0.43482657 0.36436653
 0.45933241 0.33619243 0.37674937 0.33255364 0.43905674 0.36967861
 0.36624145 0.38575829 0.42847124 0.3557334  0.39422856 0.37123186
 0.45033301 0.39553681 0.41921235 0.35355131 0.37161599 0.3806015 ]
[ 8 22  3 16 11 28  2 21 15 23  9 33  6 17 10 24 37 48 40 43 34 45 35 39
 41 46 38 47 36 44 25 42 12 32 13 26  1 19 18 31  7 29 20 27  4 14  5 30]
wind_med
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(30, 10)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-2.12324545 -2.15513237 -2.15253664 -2.11930017 -2.15009708 -2.16703571
 -2.12913043 -2.17205264 -2.10680855 -2.16528771 -2.15005533 -2.17000855
 -2.09956034 -2.15250138 -2.11904703 -2.17117864 -2.31780249 -2.4289496
 -2.26054678 -2.39949112 -2.29611098 -2.29911603 -2.19568941 -2.31939205
 -2.3305851  -2.42866102 -2.32027688 -2.41060318 -2.24505438 -2.36699291
 -2.20231041 -2.38083081 -2.10241532 -2.16766254 -2.12942526 -2.22747434
 -2.08187471 -2.16963713 -2.13839014 -2.16446165 -2.11573857 -2.17108241
 -2.1805158  -2.17864002 -2.07094698 -2.16144226 -2.11787777 -2.19412179]
[0.39150706 0.41709326 0.43836238 0.35189439 0.40775191 0.4085646
 0.39196042 0.41629427 0.4088923  0.39504477 0.39802052 0.42503874
 0.42111005 0.41815159 0.41694668 0.4210686  0.46317659 0.36967686
 0.47481808 0.3692473  0.45215371 0.39347046 0.50640998 0.36245572
 0.45050368 0.433217   0.42287304 0.34880024 0.44485549 0.42867466
 0.4055661  0.3343714  0.42750103 0.3900058  0.42408074 0.40846864
 0.42069105 0.41086679 0.42671089 0.41382108 0.42801454 0.40393317
 0.4438984  0.4150937  0.40851014 0.40292931 0.45310103 0.39210914]
[10 18 17  9 15 22 11 28  5 21 14 25  3 16  8 27 39 48 36 45 37 38 32 40
 42 47 41 46 35 43 33 44  4 23 12 34  2 24 13 20  6 26 30 29  1 19  7 31]
shww_max
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(30, 10)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.98556339 -0.99460624 -0.96264806 -0.99147699 -0.97109396 -0.97266205
 -0.95504091 -0.96240741 -0.93234537 -0.95904603 -0.95382734 -0.98862747
 -0.97111563 -1.01059696 -0.97948736 -0.95768009 -1.04081701 -1.08845686
 -1.06273685 -1.09703626 -1.01482418 -1.09290405 -1.00953997 -1.05113865
 -1.03667796 -1.08400782 -1.05705736 -1.10007361 -1.04765942 -1.06745938
 -1.02937264 -1.06807252 -0.93490667 -0.97359982 -0.95167154 -0.96159611
 -0.95822726 -0.96230421 -0.97465034 -0.96251501 -0.94845034 -0.95278396
 -0.95815719 -0.9403153  -0.93135877 -0.95855818 -0.94029226 -0.96720308]
[0.38388531 0.35946722 0.38984708 0.3915852  0.42619186 0.36385923
 0.42325697 0.41058387 0.35756241 0.36817277 0.40528484 0.40467825
 0.38790628 0.41749966 0.40843039 0.35701648 0.40897615 0.35706414
 0.43288128 0.37684585 0.40375882 0.39144485 0.39744444 0.37287147
 0.36188381 0.35360045 0.38813797 0.35782022 0.42762206 0.37361606
 0.40343763 0.39909921 0.37606629 0.38201644 0.41851267 0.39175437
 0.4365951  0.38343517 0.43241819 0.40985155 0.42785619 0.37461632
 0.4333478  0.38505665 0.40899085 0.41159991 0.42775285 0.4044406 ]
[28 31 20 30 22 24 10 18  2 15  9 29 23 33 27 11 37 45 41 47 34 46 32 39
 36 44 40 48 38 42 35 43  3 25  7 16 13 17 26 19  6  8 12  5  1 14  4 21]
shww_med
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(30, 10)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.72557267 -0.74702381 -0.71833658 -0.75146915 -0.72684842 -0.72839946
 -0.7144651  -0.75098413 -0.75130498 -0.739943   -0.73554595 -0.75857497
 -0.73218192 -0.7468208  -0.71796631 -0.74528805 -0.78016167 -0.8356571
 -0.78126613 -0.8116659  -0.77636367 -0.81462671 -0.78746147 -0.79929221
 -0.79915047 -0.81579699 -0.79747098 -0.82488189 -0.78229781 -0.79071207
 -0.78087168 -0.81554191 -0.7109348  -0.71179455 -0.70296011 -0.71689199
 -0.70736605 -0.71205367 -0.72621579 -0.75577736 -0.70724766 -0.71466773
 -0.71387577 -0.73804202 -0.71330309 -0.7108593  -0.72148674 -0.72697727]
[0.34259719 0.29802875 0.32818602 0.34091853 0.33190353 0.32317444
 0.31440681 0.34205802 0.32020691 0.30286199 0.3378574  0.35120231
 0.34168575 0.32289253 0.34902137 0.35393302 0.31785958 0.26029924
 0.32083147 0.29434791 0.28740002 0.32411147 0.34011375 0.28852145
 0.32802245 0.28744    0.33697595 0.27785801 0.29768122 0.32229951
 0.32665113 0.33059184 0.33708552 0.30668968 0.32663379 0.30606289
 0.34286156 0.32385546 0.35562178 0.36828961 0.32635424 0.32174485
 0.3478041  0.34903288 0.35844816 0.30469561 0.32816145 0.34655112]
[16 27 14 30 18 20 10 28 29 24 22 32 21 26 13 25 34 48 36 43 33 44 38 42
 41 46 40 47 37 39 35 45  5  6  1 12  3  7 17 31  2 11  9 23  8  4 15 19]
mdts_sin
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(30, 10)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.11294789 -0.13131179 -0.10620215 -0.11065683 -0.10024067 -0.11020517
 -0.09171562 -0.09268529 -0.11404316 -0.13472553 -0.10716097 -0.11251769
 -0.10300013 -0.10827497 -0.08880527 -0.09300022 -0.10807633 -0.11320245
 -0.10696956 -0.11287383 -0.10219217 -0.10640464 -0.09568653 -0.10079839
 -0.1071634  -0.11143086 -0.10298086 -0.10603238 -0.09860143 -0.1026168
 -0.09316345 -0.09132059 -0.09251941 -0.09164162 -0.09399589 -0.09372535
 -0.09364491 -0.09092815 -0.08933014 -0.08765211 -0.09210833 -0.08984216
 -0.09418914 -0.09225136 -0.09000265 -0.08904106 -0.0869614  -0.08724969]
[0.03687793 0.04830336 0.04582597 0.05150974 0.04772768 0.04090996
 0.04931305 0.04564577 0.04137414 0.04344204 0.0419925  0.03937669
 0.04491388 0.04627671 0.04615419 0.04168121 0.03978502 0.03635552
 0.04434843 0.03623756 0.04472543 0.04120743 0.04483498 0.04192028
 0.04074267 0.03693851 0.04367863 0.0389645  0.04144692 0.04330035
 0.04648886 0.04753709 0.04325004 0.04704666 0.04288335 0.04324843
 0.04470768 0.04228578 0.04658656 0.04452939 0.04526153 0.04315347
 0.04251266 0.04627248 0.04529316 0.04426205 0.04587063 0.04549361]
[44 47 32 40 25 39 12 16 46 48 35 42 30 38  4 17 37 45 34 43 27 33 23 26
 36 41 29 31 24 28 18 10 15 11 21 20 19  9  6  3 13  7 22 14  8  5  1  2]
mdts_cos
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(30, 10)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.13205679 -0.13126309 -0.11915851 -0.12542473 -0.12103112 -0.12429644
 -0.10453106 -0.10421758 -0.12819907 -0.1309126  -0.11936403 -0.12142059
 -0.11178612 -0.1175342  -0.1020869  -0.10164852 -0.12045935 -0.13176366
 -0.11449137 -0.12499683 -0.11590027 -0.12088966 -0.10909803 -0.11407075
 -0.11783034 -0.12708469 -0.11919482 -0.12431667 -0.1124901  -0.11765247
 -0.10201061 -0.10598527 -0.10558734 -0.11195198 -0.10713207 -0.11391447
 -0.10729893 -0.10484383 -0.10490636 -0.10723931 -0.10271565 -0.10731741
 -0.10676423 -0.11255934 -0.10297944 -0.104039   -0.0967901  -0.09945264]
[0.04190429 0.02913948 0.03795844 0.0488536  0.03568077 0.03604171
 0.04021447 0.04350831 0.03645359 0.03340632 0.032482   0.03511843
 0.03771991 0.03860984 0.03821951 0.04149516 0.03881952 0.03405234
 0.03883266 0.03056686 0.03706082 0.03749243 0.03913305 0.03306277
 0.03590461 0.03286566 0.03561828 0.03231188 0.03454743 0.03389865
 0.03978382 0.03960861 0.03733783 0.03377391 0.04056282 0.03607739
 0.03583191 0.03741704 0.04266802 0.04194687 0.03932205 0.03760246
 0.03859073 0.03547188 0.03966843 0.037283   0.04043984 0.04078246]
[48 46 32 42 37 39 10  9 44 45 34 38 21 29  5  3 35 47 27 41 28 36 20 26
 31 43 33 40 23 30  4 14 13 22 16 25 18 11 12 17  6 19 15 24  7  8  1  2]
shts_max
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(30, 10)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.60306063 -0.61470847 -0.60258205 -0.61551377 -0.59614505 -0.64407107
 -0.59731295 -0.60564511 -0.61257751 -0.6304561  -0.60561599 -0.61275791
 -0.5909263  -0.62307102 -0.5806707  -0.60348292 -0.66114588 -0.70965024
 -0.66438699 -0.68730707 -0.64509875 -0.66984423 -0.66548665 -0.65740065
 -0.63888324 -0.68448722 -0.66798647 -0.68021791 -0.63760544 -0.6724391
 -0.65046093 -0.67507976 -0.59246485 -0.60189678 -0.60848506 -0.63199535
 -0.58399865 -0.59700749 -0.59737007 -0.59871075 -0.58069887 -0.59621822
 -0.59948387 -0.61640166 -0.57901198 -0.59337415 -0.5843475  -0.61055397]
[0.14997831 0.12493357 0.1536981  0.15608274 0.1436358  0.17662612
 0.1697937  0.15805783 0.15917431 0.16015569 0.16009581 0.14077717
 0.15012806 0.13367061 0.15038517 0.14454088 0.15184472 0.14653752
 0.14852401 0.13479894 0.15196195 0.143935   0.16265661 0.15412964
 0.14503435 0.1570433  0.15831427 0.12937335 0.13471367 0.14107987
 0.16171703 0.14028492 0.17095277 0.14981573 0.16764454 0.15672315
 0.14669758 0.1591687  0.15858589 0.15644682 0.16318829 0.1411832
 0.15891661 0.15685667 0.15911642 0.14837355 0.1675057  0.16049549]
[18 26 17 27  9 34 12 21 24 30 20 25  6 29  2 19 38 48 39 47 35 42 40 37
 33 46 41 45 32 43 36 44  7 16 22 31  4 11 13 14  3 10 15 28  1  8  5 23]
shts_med
[{'scaler': [None, StandardScaler()], 'regressor__hidden_layer_sizes': [(30, 10)], 'regressor__activation': ['relu', 'tanh', 'logistic'], 'regressor__solver': ['adam'], 'regressor__learning_rate_init': [0.001, 0.01], 'regressor__alpha': [0.0001, 0.001], 'regressor__batch_size': ['auto', 32]}]
[-0.53339809 -0.55905216 -0.54123177 -0.56075263 -0.52910774 -0.56229869
 -0.52366643 -0.5248173  -0.53449725 -0.57526446 -0.53785606 -0.55436005
 -0.52615573 -0.55931611 -0.5205127  -0.54060102 -0.5759242  -0.59720086
 -0.56573734 -0.60481382 -0.58483526 -0.58403644 -0.57113946 -0.58879536
 -0.57145089 -0.59068564 -0.57743854 -0.60095416 -0.5665896  -0.60429778
 -0.57509606 -0.59817703 -0.51044984 -0.51912755 -0.52732454 -0.55555935
 -0.51864588 -0.52450827 -0.53233393 -0.52941834 -0.51475355 -0.53483143
 -0.51872758 -0.5379198  -0.52110166 -0.52537662 -0.5300331  -0.54364274]
[0.1116119  0.10093366 0.12556217 0.0998411  0.10999377 0.11914869
 0.10904604 0.10872934 0.11658916 0.12177927 0.10915376 0.12315977
 0.11711462 0.10692153 0.11513393 0.10776571 0.11900554 0.09972397
 0.11701541 0.1174956  0.11433593 0.1125714  0.14264598 0.10319408
 0.1094243  0.09739467 0.11449871 0.09513707 0.1185409  0.09531462
 0.1283412  0.10395275 0.11357302 0.11055304 0.11796127 0.12066696
 0.11647296 0.10369902 0.11899306 0.1202972  0.11585872 0.10967978
 0.11235495 0.11581807 0.11462735 0.10498771 0.12005717 0.12750223]
[18 28 24 30 14 31  8 10 19 37 21 26 12 29  6 23 38 44 32 48 41 40 34 42
 35 43 39 46 33 47 36 45  1  5 13 27  3  9 17 15  2 20  4 22  7 11 16 25]

*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 13087190)                   *
*                                                                           *
*****************************************************************************

Job ID: 13087190
Cluster: finisterrae3
User/Group: curso342/ulc
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 32
CPU Utilized: 1-09:56:36
CPU Efficiency: 98.72% of 1-10:22:56 core-walltime
Job Wall-clock time: 01:04:28
Memory Utilized: 8.44 GB
Memory Efficiency: 52.73% of 16.00 GB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************

